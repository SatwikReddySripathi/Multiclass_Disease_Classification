{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51FPMElM8M4b"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aISv-P98Tjn"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import ast\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.classification import MultilabelPrecision, MultilabelRecall, MultilabelF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2u446Niv8Vb2"
   },
   "outputs": [],
   "source": [
    "def load_data(original_data_pickle, batch_size, train_percent, val_percent, target_size=(224, 224)):\n",
    "    images = []\n",
    "    demographics = []\n",
    "    labels= []\n",
    "\n",
    "    resize_transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    with open(original_data_pickle, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    for item in data.values():\n",
    "\n",
    "      \"\"\"\n",
    "      The image data we get would be in bytes. We need to open it and convert it to grey scale and then resize. Recheck it. What are we doing with resizing before then?\n",
    "      \"\"\"\n",
    "      image_data = item['image_data']\n",
    "      image = Image.open(io.BytesIO(image_data)).convert('L')\n",
    "      image = resize_transform(image)  # Resizing and converting to tensor with shape (1, H, W) --> got an error without it\n",
    "\n",
    "      label= item['image_label']\n",
    "      label = ast.literal_eval(label)\n",
    "      label = np.array(label, dtype=int)\n",
    "\n",
    "      age = torch.tensor([item['age']], dtype=torch.float32)\n",
    "      gender = torch.tensor(item['gender'], dtype=torch.float32)\n",
    "\n",
    "      images.append(image)\n",
    "      demographics.append(torch.cat([age, gender]))\n",
    "      labels.append(label)\n",
    "\n",
    "    \"\"\"\n",
    "    Stacking images and demographics.\n",
    "    images Shape: (num_samples, channels, height, width)\n",
    "    demographics Shape: (num_samples, num_features)\n",
    "    \"\"\"\n",
    "    images = torch.stack(images)\n",
    "    demographics = torch.stack(demographics)\n",
    "    labels = torch.stack([torch.tensor(label, dtype=torch.long) for label in labels])\n",
    "    #labels = torch.tensor(labels, dtype= torch.long)\n",
    "\n",
    "    dataset = TensorDataset(images, demographics, labels)\n",
    "\n",
    "    train_size = int(train_percent * len(dataset))\n",
    "    val_size = int(val_percent * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, num_demographics, num_classes=15):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # Modifying the first convolutional layer to accept grayscale images (1 channel) --> generally ResNet expects 3 channels\n",
    "        #for RGB\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        # Removing the final fully connected layer in ResNet\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "\n",
    "        # this fc processes the demographics (age + gender)\n",
    "        self.demographics_fc = nn.Sequential(\n",
    "            nn.Linear(num_demographics, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(512 + 32, num_classes)  # 512 from ResNet, 32 from demographics_fc, can make it 64?\n",
    "\n",
    "    def forward(self, images, demographics):\n",
    "        x = self.resnet(images)  # Passing images through the modified ResNet (without its last layer)\n",
    "        x = x.view(x.size(0), -1)  # Flattening the ResNet output\n",
    "\n",
    "        demographics_features = self.demographics_fc(demographics)\n",
    "        x = torch.cat((x, demographics_features), dim=1)\n",
    "\n",
    "        #print(\"Shape after concatenating demographics:\", x.shape)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        #print(\"Output shape before returning:\", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sg1GtW1Z8dWX"
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"runs/CustomResNet18_experiment\")\n",
    "\n",
    "def freeze_unfreeze_layers(model, freeze=True, layers_to_train=[\"layer4\", \"fc\"]):\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(layer in name for layer in layers_to_train):\n",
    "            param.requires_grad = not freeze\n",
    "        else:\n",
    "            param.requires_grad = freeze\n",
    "\n",
    "\n",
    "def train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs= 10):\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, demographics, labels in train_loader:\n",
    "      inputs, demographics, labels = inputs.to(device), demographics.to(device), labels.to(device)\n",
    "\n",
    "      # Repeating grayscale images to make them 3 channels - this is not needed now since I changed the ResNet to accept grayscale,\n",
    "      #in-general ResNet expects RGB images ig\n",
    "\n",
    "      #inputs = inputs.repeat(1, 3, 1, 1)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = model(inputs, demographics)\n",
    "      loss = criterion(outputs, labels.float())\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "      for inputs, demographics, labels in val_loader:\n",
    "        inputs, demographics, labels = inputs.to(device), demographics.to(device), labels.to(device)\n",
    "        outputs = model(inputs, demographics)\n",
    "\n",
    "        val_loss += criterion(outputs, labels.float()).item()\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predicted = (probabilities >= 0.5).int()\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Validation\", val_accuracy, epoch)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vV4_0dnN8r4I"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(test_loader, model, criterion, precision_metric, recall_metric, f1_metric, confidence= 0.3):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for inputs, demographics, labels in test_loader:\n",
    "        inputs, demographics, labels = inputs.to(device), demographics.to(device), labels.to(device)\n",
    "        outputs = model(inputs, demographics)\n",
    "\n",
    "        test_loss += criterion(outputs, labels.float()).item()\n",
    "\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predicted = (probabilities >= confidence).int()\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "\n",
    "        #print(\"predicted:\", predicted)\n",
    "        #print(\"labels: \", labels)\n",
    "\n",
    "        precision_metric.update(predicted, labels)\n",
    "        recall_metric.update(predicted, labels)\n",
    "        f1_metric.update(predicted, labels)\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    precision = precision_metric.compute().item()\n",
    "    recall = recall_metric.compute().item()\n",
    "    f1_score = f1_metric.compute().item()\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "    writer.add_scalar(\"Loss/Test\", avg_test_loss)\n",
    "    writer.add_scalar(\"Accuracy/Test\", test_accuracy)\n",
    "    writer.add_scalar(\"Precision/Test\", precision)\n",
    "    writer.add_scalar(\"Recall/Test\", recall)\n",
    "    writer.add_scalar(\"F1-Score/Test\", f1_score)\n",
    "\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po2rD48u8v-L"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  config = {\n",
    "      \"file_path\": \"preprocessed_dummy_data.pkl\",\n",
    "      \"batch_size\": 32,\n",
    "      \"num_epochs\": 10,\n",
    "      \"learning_rate\": 1e-5,\n",
    "      \"num_demographics\": 3,\n",
    "      \"num_classes\": 15,\n",
    "      \"train_percent\": 0.7,\n",
    "      \"val_percent\": 0.1\n",
    "  }\n",
    "\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  train_loader, val_loader, test_loader = load_data(config[\"file_path\"], config[\"batch_size\"], config[\"train_percent\"], config[\"val_percent\"])\n",
    "\n",
    "  model = CustomResNet18(num_demographics=config[\"num_demographics\"], num_classes=config[\"num_classes\"]).to(device)\n",
    "  freeze_unfreeze_layers(model, freeze=True, layers_to_train=[\"layer4\", \"fc\"])\n",
    "\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config[\"learning_rate\"])\n",
    "\n",
    "  \"\"\"precision_metric = torchmetrics.Precision(average='micro').to(device)\n",
    "  recall_metric = torchmetrics.Recall(average='micro').to(device)\n",
    "  f1_metric = torchmetrics.F1Score(average='micro').to(device)\"\"\"\n",
    "\n",
    "  precision_metric = MultilabelPrecision(num_labels= config[\"num_classes\"], average='macro').to(device)\n",
    "  recall_metric = MultilabelRecall(num_labels= config[\"num_classes\"], average='macro').to(device)\n",
    "  f1_metric = MultilabelF1Score(num_labels= config[\"num_classes\"], average='macro').to(device)\n",
    "\n",
    "  train_model(train_loader, val_loader, model, criterion, optimizer, config[\"num_epochs\"])\n",
    "  evaluate_model(test_loader, model, criterion, precision_metric, recall_metric, f1_metric)\n",
    "\n",
    "  writer.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
