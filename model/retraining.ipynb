{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import ast\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "import sklearn\n",
        "import logging\n",
        "import datetime\n",
        "import itertools\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import random_split, DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "x7D7NTImSH6I"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_file= \"best_params.txt\"\n",
        "train_preprocessed_data= \"train_preprocessed_data.pkl\"\n",
        "test_preprocessed_data= \"test_preprocessed_data.pkl\"\n",
        "combined_preprocessed_data= \"combined_preprocessed_data.pkl\"\n",
        "best_model= \"new_best_model.pt\""
      ],
      "metadata": {
        "id": "G-bHJtO5VTo0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_pickles(pickle_files):\n",
        "    combined_data = {}\n",
        "\n",
        "    for file in pickle_files:\n",
        "        with open(file, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        for key, value in data.items():\n",
        "            combined_data[key] = value\n",
        "\n",
        "    return combined_data"
      ],
      "metadata": {
        "id": "cxoTWlHExUw0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(original_data_pickle, batch_size, train_percent, val_percent, target_size=(224, 224), seed= 42):\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  images = []\n",
        "  demographics = []\n",
        "  labels= []\n",
        "\n",
        "  resize_transform = transforms.Compose([\n",
        "      transforms.Resize(target_size),\n",
        "      transforms.ToTensor()\n",
        "  ])\n",
        "\n",
        "  with open(original_data_pickle, 'rb') as f:\n",
        "      data = pickle.load(f)\n",
        "\n",
        "  for item in data.values():\n",
        "\n",
        "    \"\"\"\n",
        "    The image data we get would be in bytes. We need to open it and convert it to grey scale and then resize. Recheck it. What are we doing with resizing before then?\n",
        "    \"\"\"\n",
        "    image_data = item['image_data']\n",
        "    image = Image.open(io.BytesIO(image_data)).convert('L')\n",
        "    image = resize_transform(image)  # Resizing and converting to tensor with shape (1, H, W) --> got an error without it\n",
        "\n",
        "    label= item['image_label']\n",
        "    label = ast.literal_eval(label)\n",
        "    label = np.array(label, dtype=int)\n",
        "\n",
        "    age = torch.tensor([item['age']], dtype=torch.float32)\n",
        "    gender = torch.tensor(item['gender'], dtype=torch.float32)\n",
        "\n",
        "    images.append(image)\n",
        "    demographics.append(torch.cat([age, gender]))\n",
        "    labels.append(label)\n",
        "\n",
        "  \"\"\"\n",
        "  Stacking images and demographics.\n",
        "  images Shape: (num_samples, channels, height, width)\n",
        "  demographics Shape: (num_samples, num_features)\n",
        "  \"\"\"\n",
        "  images = torch.stack(images)\n",
        "  demographics = torch.stack(demographics)\n",
        "  labels = torch.stack([torch.tensor(label, dtype=torch.long) for label in labels])\n",
        "  #labels = torch.tensor(labels, dtype= torch.long)\n",
        "\n",
        "  dataset = TensorDataset(images, demographics, labels)\n",
        "\n",
        "  train_size = int(train_percent * len(dataset))\n",
        "  #val_size = int(val_percent * len(dataset))\n",
        "  val_size = len(dataset) - train_size  # this coz it would then add the remaining images, to the val dataset else we get an error\n",
        "\n",
        "  print(f\"Train size: {train_size}, Validation size: {val_size}, length of dataset: {len(dataset)}\")\n",
        "\n",
        "  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "  print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "  return train_loader, val_loader"
      ],
      "metadata": {
        "id": "Elx_j6lt0AhU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomResNet18(nn.Module):\n",
        "    def __init__(self, demographic_fc_size, num_demographics, num_classes=15):\n",
        "        super(CustomResNet18, self).__init__()\n",
        "\n",
        "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Modifying the first convolutional layer to accept grayscale images (1 channel) --> generally ResNet expects 3 channels\n",
        "        #for RGB\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "        # Removing the final fully connected layer in ResNet\n",
        "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
        "\n",
        "        # this fc processes the demographics (age + gender)\n",
        "        self.demographics_fc = nn.Sequential(\n",
        "            nn.Linear(num_demographics, demographic_fc_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(512 + demographic_fc_size, num_classes)  # 512 from ResNet(it's how resnet is), 32 from demographics_fc, can make it 64?\n",
        "\n",
        "    def forward(self, images, demographics):\n",
        "        x = self.resnet(images)  # Passing images through the modified ResNet (without its last layer)\n",
        "        x = x.view(x.size(0), -1)  # Flattening the ResNet output\n",
        "\n",
        "        demographics_features = self.demographics_fc(demographics)\n",
        "        x = torch.cat((x, demographics_features), dim=1)\n",
        "\n",
        "        #print(\"Shape after concatenating demographics:\", x.shape)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        #print(\"Output shape before returning:\", x.shape)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "z60Yl4dA3D8l"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_unfreeze_layers(model, freeze=True, layers_to_train=[\"layer4\", \"demographics_fc\", \"fc\"]):\n",
        "    for name, param in model.named_parameters():\n",
        "        if any(layer in name for layer in layers_to_train):\n",
        "            param.requires_grad = not freeze\n",
        "        else:\n",
        "            param.requires_grad = freeze\n",
        "\n",
        "\n",
        "def train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs= 10):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  best_val_accuracy = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, demographics, labels in train_loader:\n",
        "      inputs, demographics, labels = inputs.to(device), demographics.to(device), labels.to(device)\n",
        "\n",
        "      # Repeating grayscale images to make them 3 channels - this is not needed now since I changed the ResNet to accept grayscale,\n",
        "      #in-general ResNet expects RGB images ig\n",
        "\n",
        "      #inputs = inputs.repeat(1, 3, 1, 1)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(inputs, demographics)\n",
        "      loss = criterion(outputs, labels.float())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, demographics, labels in val_loader:\n",
        "        inputs, demographics, labels = inputs.to(device), demographics.to(device), labels.to(device)\n",
        "        outputs = model(inputs, demographics)\n",
        "\n",
        "        val_loss += criterion(outputs, labels.float()).item()\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = (probabilities >= 0.5).int()\n",
        "\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.numel()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "      best_val_accuracy = val_accuracy\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}%\")\n",
        "  return best_val_accuracy"
      ],
      "metadata": {
        "id": "9A0FAsneDQNY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrain_model(train_loader, val_loader, best_params):\n",
        "  model = torch.load(best_model, map_location=device).to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  best_val_accuracy = train_model(train_loader, val_loader, model, criterion, optimizer, best_params['num_epochs'])\n",
        "  print(f\"Retrained validation accuracy: {best_val_accuracy}\")\n",
        "  return model"
      ],
      "metadata": {
        "id": "PFNOgNLq2pj5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  config = {\n",
        "      \"original_pickle\": train_preprocessed_data,\n",
        "      \"inference_pickle\": test_preprocessed_data,\n",
        "      \"combined_pickle\": combined_preprocessed_data,\n",
        "      \"num_demographics\": 3,\n",
        "      \"num_classes\": 15,\n",
        "      \"train_percent\": 0.8,\n",
        "      \"val_percent\": 0.2\n",
        "  }\n",
        "\n",
        "  combined_pickle = combine_pickles([config[\"original_pickle\"], config[\"inference_pickle\"]])\n",
        "\n",
        "  with open(config[\"combined_pickle\"], \"wb\") as f:\n",
        "    pickle.dump(combined_pickle, f)\n",
        "\n",
        "  print(\"Combined data saved to 'combined_preprocessed_data.pkl'\")\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  with open(best_params_file, \"r\") as f:\n",
        "      best_accuracy_line, params_line = f.readlines()\n",
        "      best_params = params_line.replace(\"Parameters: \", \"\").strip()\n",
        "      best_params = ast.literal_eval(best_params)\n",
        "      num_epochs, batch_size, learning_rate, demographics_fc_size = best_params\n",
        "      best_params = {\n",
        "          \"num_epochs\": num_epochs,\n",
        "          \"batch_size\": batch_size,\n",
        "          \"learning_rate\": learning_rate,\n",
        "          \"demographics_fc_size\": demographics_fc_size\n",
        "      }\n",
        "\n",
        "  print(f\"Best Params: Epochs={num_epochs}, Batch Size={batch_size}, Learning Rate={learning_rate}, Demographics FC Size={demographics_fc_size}\")\n",
        "\n",
        "  train_loader, val_loader = load_data(\n",
        "        original_data_pickle=config[\"combined_pickle\"],\n",
        "        batch_size=best_params[\"batch_size\"],\n",
        "        train_percent=0.8,\n",
        "        val_percent=0.2,\n",
        "        target_size=(224, 224)\n",
        "    )\n",
        "\n",
        "  retrained_model = retrain_model(\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        best_params=best_params\n",
        "    )\n",
        "\n",
        "  timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "  model_filename = f\"retrained_best_model_{timestamp}.pt\"\n",
        "\n",
        "  torch.save(retrained_model, model_filename)\n",
        "  print(f\"Retrained model saved as {model_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUOAR-348w0R",
        "outputId": "bdab1a30-6493-4568-f620-1cb964b191cb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined data saved to 'combined_preprocessed_data.pkl'\n",
            "Best Params: Epochs=5, Batch Size=32, Learning Rate=1e-05, Demographics FC Size=64\n",
            "Train size: 4992, Validation size: 1249, length of dataset: 6241\n",
            "Training samples: 4992, Validation samples: 1249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-73d05022769f>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(best_model, map_location=device).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Training Loss: 0.6232172686320084, Validation Loss: 0.563375823944807, Validation Accuracy: 78.50547104350147%\n",
            "Epoch 2/5, Training Loss: 0.3291966270368833, Validation Loss: 0.30780508890748026, Validation Accuracy: 88.8390712570056%\n",
            "Epoch 3/5, Training Loss: 0.30408531503799635, Validation Loss: 0.30457146614789965, Validation Accuracy: 88.81772084334133%\n",
            "Epoch 4/5, Training Loss: 0.2907921817058172, Validation Loss: 0.30685552433133123, Validation Accuracy: 88.83373365358953%\n",
            "Epoch 5/5, Training Loss: 0.27805661285916966, Validation Loss: 0.3047284599393606, Validation Accuracy: 88.97251134240726%\n",
            "Retrained validation accuracy: 88.97251134240726\n",
            "Retrained model saved as retrained_best_model_20241203_001829.pt\n"
          ]
        }
      ]
    }
  ]
}